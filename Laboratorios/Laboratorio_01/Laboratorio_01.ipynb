{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO4dKBzaWCCYKgrwKhfMDW2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Laboratorio 01\n","Contruya un mlp para clasificación de imágenes, utilizando el código de los cuadernillos proporcionados para su revisión, el dataset que utilizara será elegido por cada uno y se coordinara con el estudiante Layme Gonzales Marco Antonio, para evitar repetir, se debe realizar una descripción detallada del código desarrollado, como de los resultados obtenidos. (No se debe utilizar Pytorch)\n","\n","Es obligatorio copiar a esta plataforma el archivo py o ipynb, además de la dirección del repositorio en gitlab.\n"],"metadata":{"id":"ejzKVumS2own"}},{"cell_type":"markdown","source":["# Importacion necesaria\n","\n","*  NumPy: Para manejar matrices y cálculos numéricos.\n","*  Matplotlib: Para la visualización de datos.\n","*  Scikit-learn:\n","    * load_digits(): Carga el dataset de dígitos escritos a mano.\n","    * train_test_split(): Divide los datos en entrenamiento y prueba.\n","    * OneHotEncoder(): Convierte las etiquetas en formato one-hot.\n","\n"],"metadata":{"id":"h8GzARVy3COS"}},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import fetch_openml"],"metadata":{"id":"dNW40ZgH5Sv7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Cargar dataset de dígitos\n","\n","  * Cargamos el dataset Digits, que contiene imágenes de 8x8 píxeles representadas como vectores de 64 valores.\n","  * Normalizamos los valores de los píxeles dividiéndolos entre 255\n","  * Ajustamos la forma de las etiquetas para facilitar el procesamiento."],"metadata":{"id":"ncivr-KD3T7w"}},{"cell_type":"markdown","source":[],"metadata":{"id":"s4sWO_tk_ZMc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ix1YDQnAnUrT"},"outputs":[],"source":["# Cargar MNIST (28x28 imágenes en escala de grises)\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","X, y = mnist.data, mnist.target.astype(np.int32)\n","\n","# Normalizar imágenes (valores entre 0 y 1)\n","X = X / 255.0\n","\n","# Dividir en train/test (60k train, 10k test)\n","X_train, X_test = X[:60000], X[60000:]\n","y_train, y_test = y[:60000], y[60000:]\n"]},{"cell_type":"markdown","source":["###MLP"],"metadata":{"id":"7AIJi5gC_8oZ"}},{"cell_type":"code","source":["class MLP:\n","    def __init__(self, input_size, hidden_size, output_size):\n","        # Inicializar pesos aleatorios\n","        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n","        self.b1 = np.zeros(hidden_size)\n","        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n","        self.b2 = np.zeros(output_size)\n","\n","    def forward(self, X):\n","        # Capa oculta (ReLU)\n","        self.z1 = np.dot(X, self.W1) + self.b1\n","        self.a1 = np.maximum(0, self.z1)  # ReLU\n","\n","        # Capa de salida (Softmax)\n","        self.z2 = np.dot(self.a1, self.W2) + self.b2\n","        exp_z = np.exp(self.z2 - np.max(self.z2, axis=1, keepdims=True))\n","        self.probs = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n","        return self.probs\n","\n","    def backward(self, X, y, learning_rate):\n","        m = X.shape[0]  # Número de ejemplos\n","\n","        # Gradiente de la pérdida respecto a z2\n","        dZ2 = self.probs\n","        dZ2[range(m), y] -= 1\n","        dZ2 /= m\n","\n","        # Gradientes capa 2\n","        dW2 = np.dot(self.a1.T, dZ2)\n","        db2 = np.sum(dZ2, axis=0)\n","\n","        # Gradiente capa oculta (ReLU)\n","        dA1 = np.dot(dZ2, self.W2.T)\n","        dZ1 = dA1 * (self.z1 > 0)  # Gradiente de ReLU\n","\n","        # Gradientes capa 1\n","        dW1 = np.dot(X.T, dZ1)\n","        db1 = np.sum(dZ1, axis=0)\n","\n","        # Actualizar pesos\n","        self.W1 -= learning_rate * dW1\n","        self.b1 -= learning_rate * db1\n","        self.W2 -= learning_rate * dW2\n","        self.b2 -= learning_rate * db2\n","\n","    def train(self, X, y, epochs=100, learning_rate=0.1):\n","        for epoch in range(epochs):\n","            probs = self.forward(X)\n","            self.backward(X, y, learning_rate)\n","\n","            # Calcular pérdida y exactitud\n","            loss = -np.log(probs[range(len(y)), y]).mean()\n","            preds = np.argmax(probs, axis=1)\n","            accuracy = np.mean(preds == y)\n","\n","            if epoch % 10 == 0:\n","                print(f\"Época {epoch}, Pérdida: {loss:.4f}, Exactitud: {accuracy:.4f}\")\n","\n","    def predict(self, X):\n","        return np.argmax(self.forward(X), axis=1)"],"metadata":{"id":"65C4gpYQ6z-d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Entrenar y Evaluar"],"metadata":{"id":"Qty1RqyHACti"}},{"cell_type":"code","source":["mlp = MLP(input_size=784, hidden_size=128, output_size=10)  # MNIST: 784 (28x28), 10 clases\n","mlp.train(X_train, y_train, epochs=100, learning_rate=0.1)\n","\n","# Evaluar en test\n","test_preds = mlp.predict(X_test)\n","test_accuracy = np.mean(test_preds == y_test)\n","print(f\"Exactitud en test: {test_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H1ZvNNLGAAqi","executionInfo":{"status":"ok","timestamp":1744327337886,"user_tz":240,"elapsed":166593,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"f845e512-f25a-4bd5-a52f-2c7731b92d29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-18-13acc1ad0f84>:52: RuntimeWarning: invalid value encountered in log\n","  loss = -np.log(probs[range(len(y)), y]).mean()\n"]},{"output_type":"stream","name":"stdout","text":["Época 0, Pérdida: nan, Exactitud: 0.0000\n","Época 10, Pérdida: nan, Exactitud: 0.0000\n","Época 20, Pérdida: nan, Exactitud: 0.0000\n","Época 30, Pérdida: nan, Exactitud: 0.0000\n","Época 40, Pérdida: nan, Exactitud: 0.0000\n","Época 50, Pérdida: nan, Exactitud: 0.0000\n","Época 60, Pérdida: nan, Exactitud: 0.0000\n","Época 70, Pérdida: nan, Exactitud: 0.0000\n","Época 80, Pérdida: nan, Exactitud: 0.0000\n","Época 90, Pérdida: nan, Exactitud: 0.0000\n","Exactitud en test: 0.7828\n"]}]}]}