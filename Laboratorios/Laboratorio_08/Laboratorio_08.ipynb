{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyME3Thk1wHqawjmqohuZBO6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"aI-vvjbBRBRm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Enlace a GitHub\n","\n","https://github.com/Sebastian-Cepeda-Ch/SIS421-2025-ASCC"],"metadata":{"id":"sTQggA9wRB_h"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oUXCu4l-y3Uh","executionInfo":{"status":"ok","timestamp":1748840857527,"user_tz":240,"elapsed":2555,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"41f6f664-e846-4031-a5ab-7528302c0b08"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# Montar Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Instalar las bibliotecas necesarias\n","!pip install transformers\n","!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNBLjUazy8lf","executionInfo":{"status":"ok","timestamp":1748840864467,"user_tz":240,"elapsed":5843,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"dadad1ae-999b-4b2a-9ad1-cc3680e6dca2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"]}]},{"cell_type":"code","source":["# Importar bibliotecas\n","import torch\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from torch.optim import AdamW\n","from torch.nn import CrossEntropyLoss\n","from tqdm import tqdm"],"metadata":{"id":"wFxDlZerzEdW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función para leer párrafos de los archivos\n","def leer_parrafos(archivo):\n","    with open(archivo, 'r', encoding='utf-8') as f:\n","        contenido = f.read()\n","    parrafos = [p.strip() for p in contenido.split('*') if p.strip()]\n","    return parrafos"],"metadata":{"id":"elVkQnN70Cjy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ruta al dataset\n","ruta = '/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos'"],"metadata":{"id":"vFwKIjHGzm5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Leer los archivos y asignar etiquetas\n","parrafos_estudiantes = leer_parrafos(ruta + '/estudiantes.txt')\n","etiquetas_estudiantes = [0] * len(parrafos_estudiantes)\n","\n","parrafos_docentes = leer_parrafos(ruta + '/docentes.txt')\n","etiquetas_docentes = [1] * len(parrafos_docentes)\n","\n","parrafos_administrativos = leer_parrafos(ruta + '/administrativos.txt')\n","etiquetas_administrativos = [2] * len(parrafos_administrativos)\n","\n","parrafos_general = leer_parrafos(ruta + '/general.txt')\n","etiquetas_general = [3] * len(parrafos_general)"],"metadata":{"id":"ei33vNa1zvbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Combinar todos los párrafos y etiquetas\n","todos_parrafos = parrafos_estudiantes + parrafos_docentes + parrafos_administrativos + parrafos_general\n","todas_etiquetas = etiquetas_estudiantes + etiquetas_docentes + etiquetas_administrativos + etiquetas_general\n","\n","# Dividir en conjuntos de entrenamiento y prueba\n","train_texts, test_texts, train_labels, test_labels = train_test_split(\n","    todos_parrafos, todas_etiquetas, test_size=0.2, random_state=42\n",")"],"metadata":{"id":"Z5orT3bszyeC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definir la clase personalizada para el dataset\n","class TextoDataset(Dataset):\n","    def __init__(self, textos, etiquetas, tokenizer, max_length=512):\n","        self.textos = textos\n","        self.etiquetas = etiquetas\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.textos)\n","\n","    def __getitem__(self, idx):\n","        texto = self.textos[idx]\n","        etiqueta = self.etiquetas[idx]\n","        encoding = self.tokenizer(\n","            texto, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(etiqueta, dtype=torch.long)\n","        }"],"metadata":{"id":"asb4-C5qz1EX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cargar el tokenizador\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Crear los datasets de entrenamiento y prueba\n","train_dataset = TextoDataset(train_texts, train_labels, tokenizer)\n","test_dataset = TextoDataset(test_texts, test_labels, tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tj1R-xum0Ge1","executionInfo":{"status":"ok","timestamp":1748841197502,"user_tz":240,"elapsed":1519,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"52d26171-4f32-4075-c2a9-e051f3cd95a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Crear DataLoaders\n","train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","# Cargar el modelo BERT para clasificación\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6wfjEWnY0ImO","executionInfo":{"status":"ok","timestamp":1748841204644,"user_tz":240,"elapsed":690,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"e554e7bd-1992-4cad-863d-cb23632aa989"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# Mover el modelo a la GPU si está disponible\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bd5OM_Qe0LpW","executionInfo":{"status":"ok","timestamp":1748841210212,"user_tz":240,"elapsed":41,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"ba354b7f-8479-4013-f5b4-2758f6b1f66d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSdpaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",")"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Definir el optimizador y la función de pérdida\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","criterion = CrossEntropyLoss()"],"metadata":{"id":"-ZJ7WEhr0N8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función de entrenamiento manual\n","def train(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(train_loader, desc=\"Entrenando\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(train_loader)"],"metadata":{"id":"5ts9mdoD0SK9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Función de evaluación manual\n","def evaluate(model, test_loader, criterion, device):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for batch in tqdm(test_loader, desc=\"Evaluando\"):\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","            logits = outputs.logits\n","            preds = torch.argmax(logits, dim=1)\n","            correct += (preds == labels).sum().item()\n","            total += labels.size(0)\n","    accuracy = correct / total\n","    return total_loss / len(test_loader), accuracy"],"metadata":{"id":"-xcDccC00Weq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entrenamiento y evaluación manual\n","from pathlib import Path\n","import torch\n","\n","# Directorio para guardar checkpoints (añade esto al inicio del código)\n","CHECKPOINT_DIR = \"/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP\"\n","Path(CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)  # Crea la carpeta si no existe\n","\n","num_epochs = 3\n","best_test_loss = float('inf') # Para guardar el mejor modelo\n","\n","for epoch in range(num_epochs):\n","    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n","\n","    # Entrenamiento y evaluación\n","\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n","\n","    # --- CHECKPOINT: Guardar modelo cada época o cuando mejore la pérdida ---\n","    checkpoint_path = f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch+1}.pt\"\n","\n","    #Guardar solo si el modelo mejora (recomendado)\n","    if test_loss < best_test_loss:\n","        best_test_loss = test_loss\n","        torch.save({\n","            'epoch': epoch + 1,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'test_loss': test_loss,\n","            'test_accuracy': test_accuracy,\n","        }, f\"{CHECKPOINT_DIR}/best_model.pt\")  # Guarda solo el mejor\n","\n","    print(f\"Checkpoint guardado en {checkpoint_path}\")\n","    print(f\"Pérdida de entrenamiento: {train_loss:.4f}\")\n","    print(f\"Pérdida de prueba: {test_loss:.4f}, Precisión en prueba: {test_accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_70JsG630YTT","outputId":"c4b65439-24b8-40de-bc5d-c92ea0ab555b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/3\n"]},{"output_type":"stream","name":"stderr","text":["Entrenando:   8%|▊         | 5/63 [04:55<54:01, 55.89s/it]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"f4-gK_U-6j9j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definir las clases\n","CLASES = {\n","    0: \"Estudiantes\",\n","    1: \"Docentes\",\n","    2: \"Administrativos\",\n","    3: \"General\"\n","}\n","def inferir_clase(texto, model_path, tokenizer_path, max_length=512):\n","    \"\"\"\n","    Función para inferir la clase de un texto dado usando un modelo BERT entrenado.\n","\n","    Args:\n","        texto (str): El texto a clasificar.\n","        model_path (str): Ruta al directorio donde se guardó el modelo entrenado.\n","        tokenizer_path (str): Ruta al directorio donde se guardó el tokenizador.\n","        max_length (int): Longitud máxima de la secuencia para el tokenizador.\n","\n","    Returns:\n","        str: La clase predicha del texto.\n","    \"\"\"\n","    # Cargar el tokenizador y el modelo\n","    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n","    model = BertForSequenceClassification.from_pretrained(model_path)\n","\n","    # Mover el modelo a la GPU si está disponible\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    model.to(device)\n","\n","    # Tokenizar el texto\n","    encoding = tokenizer(\n","        texto,\n","        padding='max_length',\n","        truncation=True,\n","        max_length=max_length,\n","        return_tensors='pt'\n","    )\n","\n","    # Mover los tensores al dispositivo adecuado\n","    input_ids = encoding['input_ids'].to(device)\n","    attention_mask = encoding['attention_mask'].to(device)\n","\n","    # Realizar la inferencia\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","        prediccion = torch.argmax(logits, dim=1).item()\n","\n","    # Devolver la clase predicha\n","    return CLASES[prediccion]"],"metadata":{"id":"Qfk1HUKM0YwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Guardar el modelo entrenado (opcional)\n","model.save_pretrained('/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_model_manual')\n","tokenizer.save_pretrained('/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_tokenizer_manual')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DjUfeLys0dhV","executionInfo":{"status":"ok","timestamp":1748837260030,"user_tz":240,"elapsed":2381,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"0653db47-93e0-4047-a464-b79023bcf7e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_tokenizer_manual/tokenizer_config.json',\n"," '/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_tokenizer_manual/special_tokens_map.json',\n"," '/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_tokenizer_manual/vocab.txt',\n"," '/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_tokenizer_manual/added_tokens.json')"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["model_path = '/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_model_manual'\n","tokenizer_path = '/content/drive/MyDrive/SIS421/Laboratorios/Datasets/NLP/dataset_documentos/bert_tokenizer_manual'\n","texto_ejemplo = \"Profundizar conocimiento de actualidad socialmente útiles de su profesión para la solución de problemas técnicos consustanciados a la investigación científica básica y aplicada.\"\n","\n","clase_predicha = inferir_clase(texto_ejemplo, model_path, tokenizer_path)\n","print(f\"El texto pertenece a la clase: {clase_predicha}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yvq5AJeY0fRT","executionInfo":{"status":"ok","timestamp":1748837334175,"user_tz":240,"elapsed":2429,"user":{"displayName":"Alvaro Sebastian Cepeda Choque","userId":"18141201822039192329"}},"outputId":"89e277a9-5b2e-4a65-8e7d-22f537cd75db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["El texto pertenece a la clase: Administrativos\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","# Listas para almacenar métricas\n","train_losses = []\n","test_losses = []\n","test_accuracies = []\n","\n","for epoch in range(num_epochs):\n","    train_loss = train(model, train_loader, optimizer, criterion, device)\n","    test_loss, test_accuracy = evaluate(model, test_loader, criterion, device)\n","\n","    # Guardar métricas\n","    train_losses.append(train_loss)\n","    test_losses.append(test_loss)\n","    test_accuracies.append(test_accuracy)\n","\n","    # Guardar checkpoint (opcional)\n","    ...\n","\n","# Gráfica de pérdida\n","plt.figure(figsize=(12, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(train_losses, label='Train Loss')\n","plt.plot(test_losses, label='Test Loss')\n","plt.xlabel('Época')\n","plt.ylabel('Pérdida')\n","plt.legend()\n","\n","# Gráfica de precisión\n","plt.subplot(1, 2, 2)\n","plt.plot(test_accuracies, label='Test Accuracy', color='green')\n","plt.xlabel('Época')\n","plt.ylabel('Precisión')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"dKXEDrTN0ui6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"G_1e5goO65R1"},"execution_count":null,"outputs":[]}]}